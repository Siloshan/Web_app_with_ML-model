{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a093f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "findspark.find()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf=pyspark.SparkConf().setAppName('SparkApp').setMaster('local')\n",
    "sc=pyspark.SparkContext(conf=conf)\n",
    "spark=SparkSession(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a34b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"Merged dataset.csv\",header=True,inferSchema=True).select(\"genre\",\"lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f835c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = spark.read.csv(\"Mendeley dataset.csv\",header=True,inferSchema=True).select(\"artist_name\",\"track_name\",\"release_date\",\"genre\",\"lyrics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302690c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c0d3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|genre|              lyrics|\n",
      "+-----+--------------------+\n",
      "|  pop|hold time feel br...|\n",
      "|  pop|believe drop rain...|\n",
      "|  pop|sweetheart send l...|\n",
      "|  pop|kiss lips want st...|\n",
      "|  pop|till darling till...|\n",
      "+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f48ffff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover, CountVectorizer\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import numpy as np\n",
    "import pandas as ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c92cf94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepositions = ['aboard', 'about', 'above', 'across', 'after', 'against', 'along', 'amid', 'among', 'around', 'as', 'at', 'before', 'behind', 'below', 'beneath', 'beside', 'between', 'beyond', 'but', 'by', 'concerning', 'considering', 'despite', 'down', 'during', 'except', 'following', 'for', 'from', 'in', 'inside', 'into', 'like', 'minus', 'near', 'of', 'off', 'on', 'onto', 'opposite', 'outside', 'over', 'past', 'per', 'plus', 'regarding', 'round', 'save', 'since', 'through', 'to', 'toward', 'under', 'underneath', 'unlike', 'until', 'up', 'upon', 'versus', 'via', 'with', 'within', 'without']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fee600f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = StopWordsRemover().getStopWords() + prepositions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f76d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f5c92c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, HashingTF, StringIndexer, IndexToString\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Define your feature extraction pipeline\n",
    "tokenizer = RegexTokenizer(inputCol=\"lyrics\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "stopwords = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "prepositions = StopWordsRemover(inputCol=\"filtered\", outputCol=\"clean\", stopWords=prepositions)\n",
    "vectorizer = HashingTF(inputCol=\"clean\", outputCol=\"features\", numFeatures=10000)\n",
    "\n",
    "indexers = StringIndexer(inputCol=\"genre\",  outputCol=\"labelIndex\") # stage to convert targt variable\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, stopwords, prepositions, vectorizer,indexers])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59daf34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pipeline.fit(df).transform(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b8e2f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|genre|              lyrics|               words|            filtered|               clean|            features|labelIndex|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  pop|hold time feel br...|[hold, time, feel...|[hold, time, feel...|[hold, time, feel...|(10000,[266,668,6...|       0.0|\n",
      "|  pop|believe drop rain...|[believe, drop, r...|[believe, drop, r...|[believe, drop, r...|(10000,[309,315,3...|       0.0|\n",
      "|  pop|sweetheart send l...|[sweetheart, send...|[sweetheart, send...|[sweetheart, send...|(10000,[198,564,7...|       0.0|\n",
      "|  pop|kiss lips want st...|[kiss, lips, want...|[kiss, lips, want...|[kiss, lips, want...|(10000,[219,478,4...|       0.0|\n",
      "|  pop|till darling till...|[till, darling, t...|[till, darling, t...|[till, darling, t...|(10000,[253,493,1...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7177209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|labelIndex|            features|\n",
      "+----------+--------------------+\n",
      "|       0.0|(10000,[266,668,6...|\n",
      "|       0.0|(10000,[309,315,3...|\n",
      "|       0.0|(10000,[198,564,7...|\n",
      "|       0.0|(10000,[219,478,4...|\n",
      "|       0.0|(10000,[253,493,1...|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.select(\"labelIndex\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e739d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|  genre|\n",
      "+-------+\n",
      "|    pop|\n",
      "|  blues|\n",
      "|    rap|\n",
      "|   jazz|\n",
      "|hip hop|\n",
      "|country|\n",
      "|   rock|\n",
      "| reggae|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.select(\"genre\").distinct().show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce5daba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lable_array =  np.array(df_features.select(\"labelIndex\",\"genre\").distinct().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dbd8098",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['3.0', 'blues'],\n",
       "       ['0.0', 'pop'],\n",
       "       ['4.0', 'rock'],\n",
       "       ['1.0', 'country'],\n",
       "       ['5.0', 'jazz'],\n",
       "       ['7.0', 'hip hop'],\n",
       "       ['2.0', 'rap'],\n",
       "       ['6.0', 'reggae']], dtype='<U32')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lable_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198b5319",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a1165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ced5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2511f44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e394e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9645cf29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee2f7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4eddc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingData, testData) = df_features.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "682a8dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "dt_model = DecisionTreeClassifier(labelCol=\"labelIndex\",featuresCol=\"features\")\n",
    "model = dt_model.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3a00799f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_781a37c68953, depth=5, numNodes=57, numClasses=8, numFeatures=10000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19bd5d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction= model.transform(testData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec4c2076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+----------+\n",
      "|labelIndex|         probability|prediction|\n",
      "+----------+--------------------+----------+\n",
      "|       3.0|[0.11867935752528...|       2.0|\n",
      "|       3.0|[0.19958129797627...|       1.0|\n",
      "|       3.0|[0.11867935752528...|       2.0|\n",
      "|       3.0|[0.25190951821386...|       0.0|\n",
      "|       3.0|[0.25190951821386...|       0.0|\n",
      "|       3.0|[0.27945101029355...|       1.0|\n",
      "|       3.0|[0.23132313231323...|       0.0|\n",
      "|       3.0|[0.23132313231323...|       0.0|\n",
      "|       3.0|[0.25190951821386...|       0.0|\n",
      "|       3.0|[0.23962882096069...|       0.0|\n",
      "|       3.0|[0.25190951821386...|       0.0|\n",
      "|       3.0|[0.25190951821386...|       0.0|\n",
      "|       3.0|[0.11867935752528...|       2.0|\n",
      "|       3.0|[0.23962882096069...|       0.0|\n",
      "|       3.0|[0.23132313231323...|       0.0|\n",
      "|       3.0|[0.23132313231323...|       0.0|\n",
      "|       3.0|[0.25190951821386...|       0.0|\n",
      "|       3.0|[0.11867935752528...|       2.0|\n",
      "|       3.0|[0.25190951821386...|       0.0|\n",
      "|       3.0|[0.27945101029355...|       1.0|\n",
      "+----------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Prediction.select(\"labelIndex\",\"probability\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "570d0907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.33\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(Prediction)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "697b8d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction value for row 4: [0.25190951821386603,0.19726792009400704,0.020710928319623973,0.17802585193889542,0.12162162162162163,0.1150117508813161,0.09944183313748531,0.01601057579318449]\n"
     ]
    }
   ],
   "source": [
    "row_index = 4  # the index of the row to get the prediction value for\n",
    "prediction_column_name = \"probability\"  # the name of the prediction column\n",
    "prediction_value = Prediction.select(prediction_column_name).collect()[row_index][0]\n",
    "\n",
    "print(f\"Prediction value for row {row_index}: {prediction_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "495a9fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3251552324700894\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"labelIndex\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(Prediction)\n",
    "print(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "acd75d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|genre|              lyrics|               words|            filtered|               clean|            features|labelIndex|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "|blues|accuse peep thing...|[accuse, peep, th...|[accuse, peep, th...|[accuse, peep, th...|(10000,[394,2733,...|       3.0|\n",
      "|blues|ache head little ...|[ache, head, litt...|[ache, head, litt...|[ache, head, litt...|(10000,[290,316,3...|       3.0|\n",
      "|blues|acoustical sweetn...|[acoustical, swee...|[acoustical, swee...|[acoustical, swee...|(10000,[535,1029,...|       3.0|\n",
      "|blues|aggies aggies agg...|[aggies, aggies, ...|[aggies, aggies, ...|[aggies, aggies, ...|(10000,[166,316,3...|       3.0|\n",
      "|blues|alive dead wrong ...|[alive, dead, wro...|[alive, dead, wro...|[alive, dead, wro...|(10000,[387,419,4...|       3.0|\n",
      "+-----+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4ca495",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fae5f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8af22e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13469111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f55491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df7d1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb6c42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352fe2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900f2296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca64fe41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "073d4f1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o489.save.\n: java.io.IOException: Path DTmodel_new_for_Merged_dataset_CSV already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\r\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4160\\3633699115.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DTmodel_new_for_Merged_dataset_CSV\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark\\spark-3.2.3-bin-hadoop2.7\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    224\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;34m\"\"\"Save this ML instance to the given path, a shortcut of 'write().save(path)'.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.2.3-bin-hadoop2.7\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\ml\\util.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"path should be a string, got type %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.2.3-bin-hadoop2.7\\spark-3.2.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.2.3-bin-hadoop2.7\\spark-3.2.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark\\spark-3.2.3-bin-hadoop2.7\\spark-3.2.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o489.save.\n: java.io.IOException: Path DTmodel_new_for_Merged_dataset_CSV already exists. To overwrite it, please use write.overwrite().save(path) for Scala and use write().overwrite().save(path) for Java and Python.\r\n\tat org.apache.spark.ml.util.FileSystemOverwrite.handleOverwrite(ReadWrite.scala:683)\r\n\tat org.apache.spark.ml.util.MLWriter.save(ReadWrite.scala:167)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "model.save(\"DTmodel_new_for_Merged_dataset_CSV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966139ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be178f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81a83be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97a05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfb153",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267e8e36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a10c55d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
